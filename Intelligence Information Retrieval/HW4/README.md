# Intelligence-Informations-Retrieval-HWs
IIR home woreks in semester 1 of master course in University of Tehran

# Intelligent Information Retrieval â€“ Assignment 4 (IIRâ€‘CA4)

**Course:** Intelligent Information Retrieval  
**Assignment:** CA4 â€“ Word Association, Word Embeddings, and Learning to Rank  
**Dataset:** Cranfield Aeronautics Collection  
**Implementation:** Python  
**Date:** December 22, 2025  
**Calendar:** 2025â€‘12â€‘22 (Gregorian) | 1404/10/01 (Jalali)

---

## ğŸ“Œ Overview

This repository contains the complete implementation and analysis for **Intelligent Information Retrieval â€“ Assignment 4 (IIRâ€‘CA4)**.  
The assignment explores three core pillars of modern IR systems:

1. **Word Association**  
   - Syntagmatic relations using **Mutual Information (MI)**  
   - Paradigmatic relations using **Pseudoâ€‘Documents**

2. **Word Embeddings**  
   - Semantic representation using **GloVe**
   - Dimensionality reduction and visualization with **PCA**

3. **Learning to Rank (LTR)**  
   - Pointwise, Pairwise, and Listwise paradigms  
   - Metricâ€‘driven optimization using **LambdaRank**

All experiments are conducted on the **Cranfield Dataset**, a classical benchmark collection in Information Retrieval, widely used for evaluating retrieval models in scientific and technical domains.

---

## ğŸ“š Dataset: Cranfield Collection

The **Cranfield Collection** is a standard IR test collection consisting of:

- ~1,400 aeronautical research abstracts  
- 225 user queries  
- Expert relevance judgments (graded relevance)

**Domain:** Aerodynamics, fluid mechanics, aircraft design  

**Why Cranfield matters:**  
The dataset is small yet highly technical, making it ideal for studying:
- Vocabulary mismatch
- Term coâ€‘occurrence patterns
- Ranking effectiveness under limited data

**Reference:**  
https://ir-datasets.com/cranfield.html  

---

## ğŸ”— 1. Word Association

Word association is used to uncover latent semantic structures in the corpus beyond simple term frequency. Two complementary relationships are considered: **Syntagmatic** and **Paradigmatic**.

---

### 1.1 Syntagmatic Relations (Mutual Information)

**Definition:**  
Syntagmatic relations capture *coâ€‘occurrence* patterns â€” words that frequently appear together in context (the â€œANDâ€ relationship).

**Example:**  
â€œboundaryâ€ AND â€œlayerâ€ â†’ *boundary layer*

#### Method: Mutual Information (MI)

Mutual Information measures how much more often two words occur together compared to chance:

I(x, y) = log ( P(x, y) / (P(x) Â· P(y)) )

**Implementation Logic:**
- Build a word coâ€‘occurrence matrix using a sliding context window
- Compute joint and marginal probabilities
- Apply smoothing to avoid zero probabilities
- Rank word pairs by MI score

**Cranfield-Specific Findings:**
- Highâ€‘MI pairs identify technical collocations:
  - *Mach â€“ number*
  - *Boundary â€“ layer*
  - *Pressure â€“ distribution*

**IR Significance:**
- Improves phrase detection
- Enhances term weighting in retrieval models

**Key Reference:**  
Syntagmatic and Paradigmatic Associations in IR (Springer, 2011)  
https://link.springer.com/chapter/10.1007/978-3-642-18991-3_54  

---

### 1.2 Paradigmatic Relations (Pseudoâ€‘Documents)

**Definition:**  
Paradigmatic relations capture *substitutability* â€” words that appear in similar contexts but may not coâ€‘occur directly (the â€œORâ€ relationship).

**Example:**  
â€œairfoilâ€ OR â€œwingâ€

#### Method: Pseudoâ€‘Documents

A **pseudoâ€‘document** for a word is constructed by aggregating all its surrounding context words across the corpus.

**Implementation Steps:**
1. For each target word, collect neighboring words within a context window
2. Treat this aggregated context as a highâ€‘dimensional vector
3. Compute **Cosine Similarity** between vectors

**Cranfield-Specific Examples:**
- *airfoil* â†” *wing*
- *velocity* â†” *speed*

**IR Significance:**
- Core mechanism for **Query Expansion**
- Improves recall by resolving vocabulary mismatch

**Key Reference:**  
Paradigmatic Relation Discovery (UIUC)  
https://aclanthology.org/C02-1007.pdf  

---

## ğŸ§  2. Word Embeddings

### 2.1 GloVe (Global Vectors for Word Representation)

**GloVe** is an unsupervised embedding model that learns dense vector representations from **global coâ€‘occurrence statistics**.

**Core Insight:**  
Semantic meaning is encoded in the *ratio* of coâ€‘occurrence probabilities rather than raw counts.

**Formal Model (Pennington et al., 2014):**  
The dot product of word vectors approximates the logarithm of word coâ€‘occurrence counts.

**Why GloVe for IR?**
- Addresses vocabulary mismatch
- Captures global corpus semantics
- Particularly effective for technical domains like aeronautics

**Reference:**  
Stanford NLP â€“ GloVe Project (2014)  
https://nlp.stanford.edu/projects/glove/  

---

### 2.2 Embedding Lookup and Semantic Queries

**Implementation Highlights:**
- Preâ€‘trained GloVe vectors loaded using `gensim`
- Cosine similarity for nearestâ€‘neighbor search

**Cranfield Semantic Neighbors:**
- *supersonic* â†’ hypersonic, transonic, mach
- *airfoil* â†’ wing, camber, chord
- *turbulence* â†’ vortex, instability, flow

**IR Benefit:**  
Enables semantic search beyond exact keyword matching.

---

### 2.3 PCA Visualization

**Purpose:**  
Reduce 100â€‘dimensional vectors into 2D for qualitative evaluation.

**Tool:**  
`sklearn.decomposition.PCA`

#### Common Cranfield Clusters
- Flight regimes (supersonic, transonic, mach)
- Aeroâ€‘structures (airfoil, wing, chord)
- Fluid dynamics (turbulence, viscosity, boundary layer)
- Aerodynamic forces (lift, drag, thrust)

**ğŸ“Š Placeholder: PCA Scatter Plot**
```
![PCA Visualization of Cranfield Embeddings](images/pca_embeddings.png)
```

**Caption:**  
*â€œ2D PCA projection of GloVe word embeddings reveals distinct semantic clusters within the aerodynamics domain, validating the embedding quality for retrieval tasks.â€*

---

## ğŸ“ˆ 3. Learning to Rank (LTR)

Learning to Rank formulates retrieval as a supervised machine learning problem using relevance judgments.

---

### 3.1 Pointwise Approach

- Treats ranking as regression/classification
- Predicts absolute relevance scores for each queryâ€‘document pair

**Limitation:**  
Ignores relative ordering between documents.

---

### 3.2 Pairwise Approach

- Learns preferences between document pairs
- Minimizes ranking inversions

**Example:**  
If Doc A is more relevant than Doc B, enforce A > B

**Limitation:**  
Does not account for position importance in ranked results.

---

### 3.3 Listwise Approach

- Optimizes the ranking list as a whole
- Directly aligns with IR metrics (MAP, NDCG)

---

### 3.4 LambdaRank

**Key Innovation:**  
Optimizes nonâ€‘differentiable ranking metrics by scaling gradients with **Î”NDCG**.

**Why LambdaRank Matters:**
- Penalizes mistakes at the top of rankings
- Directly optimizes retrieval effectiveness

**Typical Implementation:**
- `XGBoost Ranker` or `LightGBM`
- Objective: `rank:ndcg`

**Reference:**  
Burges, C. (2010). *From RankNet to LambdaRank*  
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf  

---

## ğŸ“Š Evaluation Metrics

### Mean Average Precision (MAP)
- Measures ranking quality across all recall levels
- Sensitive to the order of relevant documents

### Normalized Discounted Cumulative Gain (NDCG)
- Accounts for graded relevance
- Emphasizes topâ€‘rank accuracy

**Why These Metrics:**  
They are standard for evaluating ranking systems on Cranfieldâ€‘style datasets.

---

## ğŸ› ï¸ Implementation Overview

- **Language:** Python  
- **Key Libraries:**
  - `numpy`, `scikitâ€‘learn`
  - `gensim`
  - `matplotlib`
  - `xgboost` / `lightgbm`

**Notebook:**  
`IIRâ€‘CA4â€‘Code.ipynb`

The notebook contains:
- Preprocessing pipelines
- MI and pseudoâ€‘document construction
- GloVe embedding lookup
- PCA visualization
- Learning to Rank experiments

---

## âœ… Key Takeaways

- Mutual Information effectively identifies technical collocations in scientific text.
- Pseudoâ€‘documents enable unsupervised synonym discovery for query expansion.
- GloVe embeddings successfully capture domainâ€‘specific semantics in Cranfield.
- PCA offers intuitive validation of embedding quality.
- LambdaRank provides the strongest alignment between training objectives and IR evaluation metrics (MAP, NDCG).
- Combining classical IR with modern learningâ€‘based techniques leads to substantial retrieval improvements.

---

## ğŸ“ References

- Stanford NLP â€“ GloVe (2014): https://nlp.stanford.edu/projects/glove/  
- Springer (2011): https://link.springer.com/chapter/10.1007/978-3-642-18991-3_54  
- ACL Anthology: https://aclanthology.org/C02-1007.pdf  
- Burges (2010): https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf  
- Cranfield Dataset: h-datasets.com/cranfield.html  

---

**Prepared for academic and professional portfolio use.**io use.**

